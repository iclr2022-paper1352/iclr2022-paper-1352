{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "latent_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "r-Tdf-fRYMHW",
        "EgEEvb_HlCwb",
        "nQ46S2FYUZpG",
        "m9SIY88YUmnb",
        "ExLfVSGIaIcF",
        "KztcaRPjW22Z",
        "r2pxWMkyfJ5W",
        "4tr7fiby8Z21",
        "4d8ZGgPXYIO3",
        "QK9fpwZwbaAu",
        "mjZJFg58bbK4",
        "IsNsCWi0YkqF",
        "MNzQs-Jlavoq",
        "2_zIkFpcbRde",
        "ElY2Ht8-8iNu"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Latent-Based Regression through GAN Semantics\n",
        "\n",
        "This notebook contains an example playground for the paper: LARGE: Latent-Based Regression through GAN Semantics\n"
      ],
      "metadata": {
        "id": "Z9JoOBOzUd18"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Setup required packages, models. This may take a few minutes.\n",
        "\n",
        "import os\n",
        "\n",
        "data_dir        = os.path.join(\"/content\", \"latent_regression\")\n",
        "celeba_dir      = os.path.join(data_dir, \"celeba_images\")\n",
        "latents_dir     = os.path.join(data_dir, \"latents\")\n",
        "cat_dir         = os.path.join(data_dir, \"afhq_cat_images\")\n",
        "cat_latents_dir = os.path.join(data_dir, \"cat_latents\")\n",
        "\n",
        "# install requirements\n",
        "!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html \n",
        "\n",
        "# Set up e4e for inversion\n",
        "!git clone https://github.com/omertov/encoder4editing.git\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "\n",
        "!pip install ftfy regex tqdm \n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "# Clone the repo this notebook is in.\n",
        "!git clone https://github.com/iclr2022-paper1352/iclr2022-paper-1352.git\n",
        "\n",
        "# Download data\n",
        "!gdown --id 1WqhoitsRZ2nfUslIMf5Rg_WWOp2LW2Hu\n",
        "\n",
        "!unzip -q latent_regression.zip\n",
        "!unzip -q latent_regression/celeba_images.zip -d $celeba_dir\n",
        "!unzip -q latent_regression/latents.zip -d $latents_dir\n",
        "!unzip -q latent_regression/afhq_cat_images -d $cat_dir\n",
        "!unzip -q latent_regression/afhq_cat_latents -d $cat_latents_dir\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "c42iAOHfUb2q",
        "cellView": "form"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "#@title General imports and variables\n",
        "from PIL import Image\n",
        "\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "set_feature_directions = {\"celebA\": \"precomputed_feature_dirs.npy\",\n",
        "                          \"afhq_cat\": \"precomputed_cat_feature_dirs.npy\"}\n",
        "\n",
        "set_num_layers = {\"celebA\": 18,\n",
        "                  \"afhq_cat\": 16}"
      ],
      "outputs": [],
      "metadata": {
        "id": "h8dh9ilpHo3s",
        "cellView": "form"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sorting\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r-Tdf-fRYMHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sorting in broken into two parts:\n",
        "\n",
        "\n",
        "1.   Choose a semantic direction. Currently supported options are:\n",
        "    *   Choose a direction from the paper\n",
        "    *   Create your own using a StyleCLIP-based approach\n",
        "    *   Upload a boundary file\n",
        "\n",
        "2.   Run the sorting script."
      ],
      "metadata": {
        "id": "AfBwTUAAGVNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Choose a latent direction:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EgEEvb_HlCwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Precomputed paper directions:"
      ],
      "metadata": {
        "id": "nQ46S2FYUZpG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "#@title Precomputed boundaries:\n",
        "\n",
        "boundary = 'celebA_hair_color' #@param['celebA_hair_length', 'celebA_hair_color', 'celebA_makeup', 'celebA_expression', 'cat_age', 'cat_yaw', 'cat_pitch']\n",
        "\n",
        "dataset = boundary.split(\"_\")[0]\n",
        "if dataset == \"cat\":\n",
        "    dataset = \"afhq_cat\"\n",
        "\n",
        "clip_boundary_dir = os.path.join(data_dir, \"clip_boundaries\")\n",
        "sefa_boundary_dir = os.path.join(data_dir, \"sefa_boundaries\")\n",
        "\n",
        "boundary_type = {\"celebA\": \"s\", \"afhq_cat\": \"w\"}[dataset]\n",
        "latent_src = \"e4e\"\n",
        "latent_type = {\"celebA\": \"s_latents\", \"afhq_cat\": \"latents\"}[dataset]\n",
        "boundary_ext = {\"celebA\": \".pickle\", \"afhq_cat\": \".npy\"}[dataset]\n",
        "\n",
        "boundary_path = {\"celebA_hair_length\": os.path.join(clip_boundary_dir, \"short_to_long_S_10\"),\n",
        "                 \"celebA_hair_color\":  os.path.join(clip_boundary_dir, \"black_to_blonde_S\"),\n",
        "                 \"celebA_makeup\":      os.path.join(clip_boundary_dir, \"makeup_S\"),\n",
        "                 \"celebA_expression\":  os.path.join(clip_boundary_dir, \"sad_to_smile_S\"),\n",
        "                 \"cat_age\":            os.path.join(sefa_boundary_dir, \"cat_age\"),\n",
        "                 \"cat_yaw\":            os.path.join(sefa_boundary_dir, \"cat_yaw\"),\n",
        "                 \"cat_pitch\":          os.path.join(sefa_boundary_dir, \"cat_pitch\")}[boundary]\n",
        "\n",
        "boundary_path = os.path.join(boundary_path, f'boundary{boundary_ext}')"
      ],
      "outputs": [],
      "metadata": {
        "id": "m_o7IUGMSTRu",
        "cellView": "form"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create your own direction with StyleCLIP:\n",
        "\n",
        "*   Enter source and target texts which describe the sorting direction.\n",
        "*   Choose a cutoff value for considered directions. A higher value gives a more disentangled direction, at the risk of removing too much relevant information.\n",
        "\n"
      ],
      "metadata": {
        "id": "m9SIY88YUmnb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Generate a StyleCLIP boundary:\n",
        "\n",
        "dataset = 'afhq_cat' #@param['celebA', 'afhq_cat']\n",
        "\n",
        "source_text = \"white fur\" #@param {type: \"string\"}\n",
        "target_text = \"black fur\" #@param {type: \"string\"}\n",
        "\n",
        "cutoff_percentile = 80 #@param {type: \"integer\"}\n",
        "\n",
        "boundary_type = \"w+\"\n",
        "latent_src = \"psp\"\n",
        "latent_type = \"latents\"\n",
        "boundary_ext = \".npy\"\n",
        "clip_boundary_dir = os.path.join(data_dir, \"clip_boundaries\")\n",
        "\n",
        "feature_dir_file = set_feature_directions[dataset]\n",
        "\n",
        "num_layers = set_num_layers[dataset]\n",
        "\n",
        "feature_dirs_file = os.path.join(clip_boundary_dir, feature_dir_file)\n",
        "\n",
        "boundary = \"live_editing\"\n",
        "\n",
        "!python LARGE/get_clip_boundary.py --source_text \"$source_text\" --target_text \"$target_text\" --name $boundary --precomputed_dirs $feature_dirs_file --out_dir $clip_boundary_dir --cutoff_percentile $cutoff_percentile --model_layers $num_layers\n",
        "\n",
        "boundary_path = os.path.join(clip_boundary_dir, boundary, f\"boundary{boundary_ext}\")\n",
        "\n",
        "print(f\"Done! The generated boundary can be found in: {boundary_path}\")\n",
        "\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "bo8XRRpCSZBt",
        "cellView": "form"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload your own boundary:\n",
        "\n",
        "The boundary should describe a direction in one of the latent spaces of StyleGAN2. We currently support boundaries in a numpy (npy) and pickle format, and in either W, W+ or S space."
      ],
      "metadata": {
        "id": "ExLfVSGIaIcF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "dataset = 'afhq_cat' #@param['celebA', 'afhq_cat']\n",
        "\n",
        "boundary_path = \"\" #@param {type: \"string\"}\n",
        "\n",
        "boundary_type = \"w\" #@param['w', 'w+', 's']\n",
        "latent_src  = \"psp\" if boundary_type == \"w+\" else \"e4e\"\n",
        "latent_type = \"latents\""
      ],
      "outputs": [],
      "metadata": {
        "id": "DIU_nJKaaHsw",
        "cellView": "form"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Sort!\n",
        "\n",
        "Pick a seed and run the block in order to sort a random subset of CelebA according to your chosen semantic boundary."
      ],
      "metadata": {
        "id": "KztcaRPjW22Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Sort with the chosen boundary:\n",
        "\n",
        "seed = 2 #@param {type: \"integer\"}\n",
        "high_variance_samples = True #@param {type:\"boolean\"}\n",
        "\n",
        "latents = cat_latents_dir if dataset == \"afhq_cat\" else os.path.join(latents_dir, latent_src, dataset, latent_type)\n",
        "images  = cat_dir if dataset == \"afhq_cat\" else celeba_dir\n",
        "\n",
        "output_dir = os.path.join(\"sorted\", boundary)\n",
        "\n",
        "if high_variance_samples:\n",
        "  weighted_edges = True\n",
        "  num_samples = 100\n",
        "else:\n",
        "  weighted_edges = False\n",
        "  num_samples = 10\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "num_layers = set_num_layers[dataset]\n",
        "\n",
        "to_wp_flag = \"--boundary_to_wp\" if boundary_type == 'w' else ''\n",
        "!python LARGE/sort.py --latent_path $latents --image_path $images --boundary_path $boundary_path --num_samples $num_samples --out_dir $output_dir --seed $seed --num_for_plot 10 $to_wp_flag --model_layers $num_layers --weighted_edges $weighted_edges\n",
        "Image.open(os.path.join(output_dir, \"all_sorted.jpg\"))\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "_e56nDdPZCwL",
        "cellView": "form"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression"
      ],
      "metadata": {
        "id": "r2pxWMkyfJ5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For regression - first train a model using labeled CelebA data, then run inference on images from the CelebA test split or an uploaded image of your own."
      ],
      "metadata": {
        "id": "CtWmN7YXFBru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a regression model\n",
        "\n",
        "We currently support pose and age models, using the labels from the paper"
      ],
      "metadata": {
        "id": "4tr7fiby8Z21"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model_type = 'pose' #@param['pose', 'age']\n",
        "\n",
        "latents_path = os.path.join(latents_dir, \"e4e\", \"celebA\", \"latents\")\n",
        "\n",
        "annotation_file = {\"pose\": \"celeba_yaw.csv\", \"age\": \"celeba_age.csv\"}[model_type]\n",
        "boundary_path = {\"pose\": \"pose_boundary\", \"age\": \"clip_boundaries/old_to_young_WP\"}[model_type]\n",
        "attribute = {\"pose\": \"yaw\", \"age\": \"Young\"}[model_type]\n",
        "\n",
        "w_boundary_flags = f\"--boundary_to_wp --layer_weights_path {data_dir}/{boundary_path}/layer_weights.npy\" if model_type == \"pose\" else \"\"\n",
        "distance_type_flags = \"\" if model_type == \"pose\" else \"--distance_type euclidean\"\n",
        "\n",
        "!python LARGE/train_regression_model.py --data-path $latents_path --annotations-file $data_dir/labels/$annotation_file --boundary_path $data_dir/$boundary_path $w_boundary_flags --attribute $attribute --output-dir $data_dir/regression_models/ --regularization L1 L2 --feature_sample_ratio 0.95 $distance_type_flags"
      ],
      "outputs": [],
      "metadata": {
        "id": "mtm9EJXYfjmp",
        "cellView": "form"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Invert an image into the latent space of the GAN, or choose a pre-inverted image:"
      ],
      "metadata": {
        "id": "4d8ZGgPXYIO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Choose a pre-inverted image (skip if you want to invert your own)"
      ],
      "metadata": {
        "id": "QK9fpwZwbaAu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "image_files = os.listdir(celeba_dir)\n",
        "image_paths = [os.path.join(celeba_dir, file_name) for file_name in image_files]\n",
        "\n",
        "latent_type = \"w+\" #@param['w+', 's']\n",
        "latent_dir_name = \"s_latents\" if latent_type == 's' else \"latents\"\n",
        "\n",
        "latents = [os.path.join(latents_dir, \"e4e\", \"celebA\", latent_dir_name, file_name.split(\".\")[0] + \".pickle\") for file_name in image_files]\n",
        "\n",
        "image_id = 731 #@param {type:\"slider\", min:1, max:4500, step:1}\n",
        "\n",
        "chosen_latent = latents[image_id - 1]\n",
        "chosen_img = image_paths[image_id - 1]\n",
        "\n",
        "img = Image.open(chosen_img)\n",
        "img.resize((256, 256))"
      ],
      "outputs": [],
      "metadata": {
        "id": "RGwFcLyMbeRx",
        "cellView": "form"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Invert an image (skip if you picked a pre-inverted image)"
      ],
      "metadata": {
        "id": "mjZJFg58bbK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Setup e4e for inversion. Skip if you want to use a pre-inverted image."
      ],
      "metadata": {
        "id": "IsNsCWi0YkqF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title e4e setup\n",
        "e4e_dir = os.path.join(\"/content\", \"encoder4editing\")\n",
        "sys.path.append(e4e_dir)\n",
        "\n",
        "from models.psp import pSp\n",
        "from gdown import download as drive_download\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from argparse import Namespace\n",
        "\n",
        "e4e_model_path = os.path.join(e4e_dir, \"e4e_ffhq_encode.pt\")\n",
        "if not os.path.isfile(e4e_model_path):\n",
        "    drive_download(\"https://drive.google.com/uc?id=1O8OLrVNOItOJoNGMyQ8G8YRTeTYEfs0P\", e4e_model_path, quiet=False)\n",
        "experiment_type = 'ffhq_encode'\n",
        "\n",
        "# os.chdir('/content/encoder4editing')\n",
        "\n",
        "EXPERIMENT_ARGS = {\n",
        "        \"model_path\": e4e_model_path\n",
        "    }\n",
        "EXPERIMENT_ARGS['transform'] = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "resize_dims = (256, 256)\n",
        "\n",
        "model_path = EXPERIMENT_ARGS['model_path']\n",
        "ckpt = torch.load(model_path, map_location='cpu')\n",
        "opts = ckpt['opts']\n",
        "\n",
        "# update the training options\n",
        "opts['checkpoint_path'] = model_path\n",
        "opts = Namespace(**opts)\n",
        "e4e_net = pSp(opts)\n",
        "e4e_net.eval()\n",
        "e4e_net.cuda()\n",
        "print('Model successfully loaded!')"
      ],
      "outputs": [],
      "metadata": {
        "id": "9E8IwEAmYGxm",
        "cellView": "form"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Choose and align an image for inversion. Skip if you want to use a pre-inverted image."
      ],
      "metadata": {
        "id": "MNzQs-Jlavoq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Align image\n",
        "image_path = \"latent_regression/sample_image/sample_steph.jpg\" #@param {type: \"string\"}\n",
        "original_image = Image.open(image_path)\n",
        "original_image = original_image.convert(\"RGB\")\n",
        "if experiment_type == \"ffhq_encode\" and 'shape_predictor_68_face_landmarks.dat' not in os.listdir():\n",
        "    !wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "    !bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "def run_alignment(image_path):\n",
        "  import dlib\n",
        "  from utils.alignment import align_face\n",
        "  predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "  aligned_image = align_face(filepath=image_path, predictor=predictor) \n",
        "  print(\"Aligned image has shape: {}\".format(aligned_image.size))\n",
        "  return aligned_image \n",
        "\n",
        "if experiment_type == \"ffhq_encode\":\n",
        "  input_image = run_alignment(image_path)\n",
        "else:\n",
        "  input_image = original_image\n",
        "\n",
        "input_image.resize(resize_dims)"
      ],
      "outputs": [],
      "metadata": {
        "id": "hw6BWJzzax8N",
        "cellView": "form"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Invert the aligned image. Skip if you want to use a pre-inverted image."
      ],
      "metadata": {
        "id": "2_zIkFpcbRde"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "#@title Invert the image\n",
        "\n",
        "from utils.common import tensor2im\n",
        "\n",
        "img_transforms = EXPERIMENT_ARGS['transform']\n",
        "transformed_image = img_transforms(input_image)\n",
        "\n",
        "def display_alongside_source_image(result_image, source_image):\n",
        "    res = np.concatenate([np.array(source_image.resize(resize_dims)),\n",
        "                          np.array(result_image.resize(resize_dims))], axis=1)\n",
        "    return Image.fromarray(res)\n",
        "\n",
        "def run_on_batch(inputs, net):\n",
        "    images, latents = net(inputs.to(\"cuda\").float(), randomize_noise=False, return_latents=True)\n",
        "    if experiment_type == 'cars_encode':\n",
        "        images = images[:, :, 32:224, :]\n",
        "    return images, latents\n",
        "\n",
        "with torch.no_grad():\n",
        "    images, latents = run_on_batch(transformed_image.unsqueeze(0), e4e_net)\n",
        "    result_image, latent = images[0], latents[0]\n",
        "\n",
        "print(latents.size())\n",
        "\n",
        "chosen_latent = os.path.join(\"/content\", \"latent.npy\")\n",
        "chosen_img = image_path\n",
        "\n",
        "np.save(chosen_latent, latents.detach().cpu().numpy())\n",
        "\n",
        "# Display inversion:\n",
        "display_alongside_source_image(tensor2im(result_image), input_image)"
      ],
      "outputs": [],
      "metadata": {
        "id": "zMAfmxGrbD7V",
        "cellView": "form"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict semantic property strength using the trained regression model"
      ],
      "metadata": {
        "id": "ElY2Ht8-8iNu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "num_training_images = 1000 #@param[2, 5, 10, 20, 1000]\n",
        "\n",
        "model_file = f\"model_{num_training_images}.pickle\"\n",
        "\n",
        "!python LARGE/infer.py --pretrained_model $data_dir/regression_models/$model_file --latent_path \"$chosen_latent\" --boundary_path $data_dir/$boundary_path/boundary.npy $w_boundary_flags $distance_type_flags"
      ],
      "outputs": [],
      "metadata": {
        "id": "Ge261zPL8uwK",
        "cellView": "form"
      }
    }
  ]
}